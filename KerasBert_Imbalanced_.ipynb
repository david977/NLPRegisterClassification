{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KerasBert_Imbalanced.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE1XoS5cQX_y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61c8e4a2-5dac-4720-ac46-07e5c17dcea6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kekwzNl1nRPM"
      },
      "source": [
        "###**Package Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ItjdKxEQygl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2089236-8c67-4867-ae2a-f0846b7a6ca0"
      },
      "source": [
        "!pip install keras-bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-bert\n",
            "  Downloading https://files.pythonhosted.org/packages/e2/7f/95fabd29f4502924fa3f09ff6538c5a7d290dfef2c2fe076d3d1a16e08f0/keras-bert-0.86.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-bert) (1.18.5)\n",
            "Requirement already satisfied: Keras>=2.4.3 in /usr/local/lib/python3.6/dist-packages (from keras-bert) (2.4.3)\n",
            "Collecting keras-transformer>=0.38.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/6c/d6f0c164f4cc16fbc0d0fea85f5526e87a7d2df7b077809e422a7e626150/keras-transformer-0.38.0.tar.gz\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras-bert) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras-bert) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras-bert) (3.13)\n",
            "Collecting keras-pos-embd>=0.11.0\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.27.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/32/45adf2549450aca7867deccfa04af80a0ab1ca139af44b16bc669e0e09cd/keras-multi-head-0.27.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.14.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/57/ef/61a1e39082c9e1834a2d09261d4a0b69f7c818b359216d4e1912b20b1c86/keras-embed-sim-0.8.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras>=2.4.3->keras-bert) (1.15.0)\n",
            "Collecting keras-self-attention==0.46.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/6b/c804924a056955fa1f3ff767945187103cfc851ba9bd0fc5a6c6bc18e2eb/keras-self-attention-0.46.0.tar.gz\n",
            "Building wheels for collected packages: keras-bert, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.86.0-cp36-none-any.whl size=34145 sha256=4d6bf3297c43e4e315f240c9a9769beefe06bc6bc2a93e59ff5ec392d58bf473\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/f0/b1/748128b58562fc9e31b907bb5e2ab6a35eb37695e83911236b\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.38.0-cp36-none-any.whl size=12942 sha256=68b8849a53503f97cf2a3ad4484912b0d48600c9ce4056160a5ca016cdb7e511\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/fb/3a/37b2b9326c799aa010ae46a04ddb04f320d8c77c0b7e837f4e\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7554 sha256=2de03c6dfdae1c1f8492d8a8582a0e48f16b300166794dde5acb15cc51348b80\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.27.0-cp36-none-any.whl size=15612 sha256=3c5901fdbdc5b36da8bda630f58f3f9a75984067f912f4a50f9850019e60fb61\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b4/49/0a0c27dcb93c13af02fea254ff51d1a43a924dd4e5b7a7164d\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=37c1f50a2c87e8816fb47c352727ac8de3c918eed240d0ddac3b01c86405c3e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5626 sha256=9d5b3b9dc25f8148c2766a3247baca49eefc1c39a79e636668604095343add01\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.8.0-cp36-none-any.whl size=4559 sha256=a2207724f32e72efd12f9aeab527e38ef66a8f96feb6982c177b01ec0711f6a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/45/8b/c111f6cc8bec253e984677de73a6f4f5d2f1649f42aac191c8\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.46.0-cp36-none-any.whl size=17278 sha256=d350c25b36790f383a9ce25f280a2ae448a2e70fe950b4e6f3bd9b6f3522201f\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/2e/80/fec4c05eb23c8e13b790e26d207d6e0ffe8013fad8c6bdd4d2\n",
            "Successfully built keras-bert keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert\n",
            "Successfully installed keras-bert-0.86.0 keras-embed-sim-0.8.0 keras-layer-normalization-0.14.0 keras-multi-head-0.27.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.46.0 keras-transformer-0.38.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PM8YtZhnbeS"
      },
      "source": [
        "###**Tensorflow Configuration**\n",
        "\n",
        "We setup an environment variable for keras-bert to use tensorflow.python.keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWulnnIaQ4Bf"
      },
      "source": [
        "import os\n",
        "os.environ['TF_KERAS'] = '1'    # Required to use tensorflow.python.keras with keras-bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-Jtw4gXQ7cD"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo-qH8PanyQA"
      },
      "source": [
        "###**Settingup GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAAW4Lm1TFbJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd0ffb60-f9f2-4164-d99a-0e2761b73701"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "    print('We will use the GPU:', device_name)\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "We will use the GPU: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU1yUKKMoX96"
      },
      "source": [
        "###**Load Train and Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH0vPA--TGQm"
      },
      "source": [
        "train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/train_file.txt', sep='{}{}{}', engine = 'python')\n",
        "test = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/test_file.txt\", sep= '{}{}{}', engine = 'python')\n",
        "\n",
        "#from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, val =  train,test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coe-uY3mocHk"
      },
      "source": [
        "###**Download Pretrained BERT Model**\n",
        "Here I have downloaded the Large Cased trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgzJxqytZjrU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43072852-2cd8-4cf9-eab8-d3cf9e5c166f"
      },
      "source": [
        "# Give -nc (--no-clobber) argument so that the file isn't downloaded multiple times \n",
        "!wget -nc https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-01 18:16:15--  https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.212.128, 172.217.214.128, 108.177.111.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.212.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 404261442 (386M) [application/zip]\n",
            "Saving to: ‘cased_L-12_H-768_A-12.zip’\n",
            "\n",
            "cased_L-12_H-768_A- 100%[===================>] 385.53M  72.5MB/s    in 5.7s    \n",
            "\n",
            "2020-12-01 18:16:21 (67.2 MB/s) - ‘cased_L-12_H-768_A-12.zip’ saved [404261442/404261442]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sq-OtNp0aCPr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62e71c4e-eb04-4da9-b017-db44693e506f"
      },
      "source": [
        "# Give -n argument so that existing files aren't overwritten \n",
        "!unzip -n cased_L-12_H-768_A-12.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  cased_L-12_H-768_A-12.zip\n",
            "   creating: cased_L-12_H-768_A-12/\n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: cased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paCYkVC3owew"
      },
      "source": [
        "- vocab.txt is a plain file listing vocabulary items \n",
        "- bert_config.json consists of model configuration in JSON format\n",
        "- bert_model.ckpt.* consists of model checkpoint data with pretrained weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mF5quhtaK4p"
      },
      "source": [
        "bert_vocab_path = 'cased_L-12_H-768_A-12/vocab.txt'\n",
        "bert_config_path = 'cased_L-12_H-768_A-12/bert_config.json'\n",
        "bert_checkpoint_path = 'cased_L-12_H-768_A-12/bert_model.ckpt'    # suffixes not required\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXMipkcbtp2D"
      },
      "source": [
        "Make sure if the model we downloaded was case sensitive or not"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtKAgmlsad8n"
      },
      "source": [
        "model_is_cased = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT5ZlsK-tzCi"
      },
      "source": [
        "Shuffle the data to avoid bias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_waBxcuVp_AY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "9c6e32a8-0a24-443e-9775-25b7cde93332"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "train = shuffle(train)\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>16937</th>\n",
              "      <td>__label__sl</td>\n",
              "      <td>I Think You Know What I Mean Lyrics  Lyle Love...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>932</th>\n",
              "      <td>__label__ob</td>\n",
              "      <td>When should we commemorate the centenary of cy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8086</th>\n",
              "      <td>__label__pb</td>\n",
              "      <td>I was in Starbucks the other day catching up w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3724</th>\n",
              "      <td>__label__ne</td>\n",
              "      <td>A water bear (Paramacrobiotus craterlaki). Sci...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6178</th>\n",
              "      <td>__label__ne</td>\n",
              "      <td>Wind Could Supply 1/10th of the World's Power ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Label                                               Text\n",
              "16937  __label__sl  I Think You Know What I Mean Lyrics  Lyle Love...\n",
              "932    __label__ob  When should we commemorate the centenary of cy...\n",
              "8086   __label__pb  I was in Starbucks the other day catching up w...\n",
              "3724   __label__ne  A water bear (Paramacrobiotus craterlaki). Sci...\n",
              "6178   __label__ne  Wind Could Supply 1/10th of the World's Power ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJjgZSoSt2hv"
      },
      "source": [
        "###**Load BERT Vocabulary**\n",
        "A plain text file with one vocabulary item per line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEoG07Iiak7R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "074ceead-3b37-4680-8a5b-b290c6fb8b75"
      },
      "source": [
        "vocabulary = []\n",
        "with open(bert_vocab_path) as f:\n",
        "    for i, line in enumerate(f):\n",
        "        vocabulary.append(line.rstrip('\\n'))    # rstrip to remove newline characters\n",
        "\n",
        "\n",
        "# Print a list with every 500th vocabulary item\n",
        "print(vocabulary[0::500])\n",
        "print(len(vocabulary))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[PAD]', 'щ', '吉', 'told', 'space', 'operations', 'proposed', 'Oxford', 'showing', 'domestic', 'mountains', 'commission', 'voices', 'associate', 'hills', 'Guide', 'relaxed', 'Page', 'Heights', 'singers', 'Interior', 'considers', 'facilitate', 'shouting', '1826', 'constitute', 'alter', 'clip', 'Into', 'Memory', 'ballad', 'Owens', 'Langdon', 'aquatic', 'stereo', 'Cass', 'Shock', '195', '##tec', '##sonic', 'attested', '##rdes', '1840s', '##90', 'Guys', '##rien', 'Munro', 'Ursula', 'mesh', 'diplomacy', 'Newmarket', '##oughs', 'synthesizers', 'Drugs', 'monstrous', '##ynamic', 'troll', '##ٹ']\n",
            "28996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG2VctbAu9Ws"
      },
      "source": [
        "###**Load BERT Configuration File**\n",
        "The configuration is just a json file so we use json.load from python json library. We wont actually need to use these configuration details directly (keras-bert takes care of them for us). Lets see what information is contained in the config file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e83kWLoDaxs1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee37d8ae-c6c6-442f-f26d-89d1c218d62d"
      },
      "source": [
        "from pprint import pprint    # pretty-printer for output\n",
        "import json\n",
        "\n",
        "with open(bert_config_path) as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "\n",
        "# Print configuration contents\n",
        "pprint(config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'attention_probs_dropout_prob': 0.1,\n",
            " 'hidden_act': 'gelu',\n",
            " 'hidden_dropout_prob': 0.1,\n",
            " 'hidden_size': 768,\n",
            " 'initializer_range': 0.02,\n",
            " 'intermediate_size': 3072,\n",
            " 'max_position_embeddings': 512,\n",
            " 'num_attention_heads': 12,\n",
            " 'num_hidden_layers': 12,\n",
            " 'type_vocab_size': 2,\n",
            " 'vocab_size': 28996}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeDs_skhhliF"
      },
      "source": [
        "We can see dropout probability, hidden size, number of hidden layers, vocabulary size and many other parameters above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbFHGVfOwRhP"
      },
      "source": [
        "###**Create BERT Tokenizer**\n",
        "\n",
        "To create the tokenizer, we'll need mapping from vocabulary items to their integer indices. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-DF4OVhbIoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dcdd68a-aa3a-4df6-dc27-60d94c2fb141"
      },
      "source": [
        "import random\n",
        "# Create mapping from vocabulary items to their indices in the vocabulary\n",
        "token_dict = { value: i for i, value in enumerate(vocabulary) }\n",
        "\n",
        "\n",
        "# Print some random examples of the mapping\n",
        "pprint(dict(random.choices(list(token_dict.items()), k=10)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'##ald': 18728,\n",
            " '##rts': 13245,\n",
            " '##tating': 24558,\n",
            " 'Opening': 13902,\n",
            " 'alley': 10959,\n",
            " 'dedication': 13314,\n",
            " 'forcibly': 23129,\n",
            " 'gene': 5565,\n",
            " 'shape': 3571,\n",
            " 'werewolf': 14665}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V1LA4AhyfJL"
      },
      "source": [
        "We'll use the keras-bert Tokenizer for BERT tokenization. The implementation supports\n",
        "\n",
        "- (Optional) lowercasing: Hello → hello\n",
        "- Basic tokenization: Hello! → Hello !, multi-part → multi - part\n",
        "- Wordpiece tokenization: comprehensively → comprehensive ##ly\n",
        "- Adding special tokens: Sentence → [CLS] Sentence [SEP]\n",
        "- Mapping to integer indices\n",
        "- Generating segment sequence\n",
        "- (Optional) padding and truncation to length\n",
        "\n",
        "In the following example, notice how words not in the dictionary are broken up into subwords (with continuation parts starting with ##) and how unknown characters are mapped to a special unknown word token [UNK].\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXCre_XJbn3U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec50310f-b72e-4a39-aa3b-0b27de34433c"
      },
      "source": [
        "from keras_bert import Tokenizer\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(token_dict, cased=model_is_cased)\n",
        "\n",
        "\n",
        "# Let's test that out\n",
        "for s in ['I am doing NLP thesis :) 汉']:\n",
        "    print('Original string:', s)\n",
        "    print('Tokenized:', tokenizer.tokenize(s))\n",
        "    indices, segments = tokenizer.encode(s, max_len=20)    # max_len for padding and truncation\n",
        "    print('Encoded:', indices)\n",
        "    print('Segments:', segments)\n",
        "    print('Decoded:', ' '.join(tokenizer.decode(indices)))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original string: I am doing NLP thesis :) 汉\n",
            "Tokenized: ['[CLS]', 'I', 'am', 'doing', 'NL', '##P', 'thesis', ':', ')', '汉', '[SEP]']\n",
            "Encoded: [101, 146, 1821, 1833, 21239, 2101, 9593, 131, 114, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Segments: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Decoded: I am doing NL ##P thesis : ) [UNK]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opoaePkghOxv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU6VNxAag9MG"
      },
      "source": [
        "# max_le = 0\n",
        "# i = 0\n",
        "# for i, document in enumerate (train['Text'].values):\n",
        "#   tokenized = tokenizer.tokenize(document)\n",
        "#   document_length = len(tokenized)\n",
        "\n",
        "#   if document_length > max_le:\n",
        "\n",
        "#     max_le = document_length\n",
        "#     i = i\n",
        "\n",
        "# print(max)\n",
        "# print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJa3IR_qmduv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fa13787-f5d9-4d85-a6b1-34bd7c56264f"
      },
      "source": [
        "#print(train['Text'].values[17587])\n",
        "len(tokenizer.tokenize(train['Text'].values[17587]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "218"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3x_dewNeAA4"
      },
      "source": [
        "train.head()\n",
        "train['Label'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljYGSct308Tk"
      },
      "source": [
        "###**Vectorize data**\n",
        "Using Label Encoder to Vectorize the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blKQQb4abuxB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f74993d-ea7b-4a11-ed72-e6c2165dd89c"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "labels =train['Label'].values\n",
        "label_encoder = LabelEncoder()    # Turns class labels into integers\n",
        "Y = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Take note of how many unique labels there are in the data\n",
        "num_labels = len(set(Y))\n",
        "\n",
        "\n",
        "# Print out some examples\n",
        "print('Number of unique labels:', num_labels)\n",
        "print(type(labels), labels[:10])\n",
        "print(type(Y), Y[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique labels: 26\n",
            "<class 'numpy.ndarray'> ['__label__sl' '__label__ob' '__label__pb' '__label__ne' '__label__ne'\n",
            " '__label__ne' '__label__ne' '__label__ne' '__label__ib' '__label__ib']\n",
            "<class 'numpy.ndarray'> [21 13 14 12 12 12 12 12 10 10]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RT-ni8dp7Nk"
      },
      "source": [
        "y_val = label_encoder.fit_transform(test['Label'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2EtoHqs6Fpy"
      },
      "source": [
        "Keep token indices and segment ids in separate numpy arrays. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJU1bDLpe-dj"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "train_token_indices, train_segment_ids = [], []  #bert tokenizer indices and their segment ids  (to separate sequences)\n",
        "val_token_indices, val_segment_ids = [], []\n",
        "for text in train['Text'].values:\n",
        "    # tokenizer.encode() returns a sequence of token indices\n",
        "    # and a sequence of segment IDs. BERT expects both as input,\n",
        "    # even if the segments IDs are just all zeros (like here).\n",
        "    ttid, tsid = tokenizer.encode(text, max_len=256)\n",
        "    train_token_indices.append(ttid)\n",
        "    train_segment_ids.append(tsid)\n",
        " \n",
        "for text in test['Text'].values:\n",
        "    # tokenizer.encode() returns a sequence of token indices\n",
        "    # and a sequence of segment IDs. BERT expects both as input,\n",
        "    # even if the segments IDs are just all zeros (like here).\n",
        "    vtid, vsid = tokenizer.encode(text, max_len=256)\n",
        "    val_token_indices.append(vtid)\n",
        "    val_segment_ids.append(vsid)\n",
        "\n",
        "# Format input as list of two numpy arrays\n",
        "train_X = [np.array(train_token_indices), np.array(train_segment_ids)]\n",
        "val_X = [np.array(val_token_indices), np.array(val_segment_ids)]\n",
        "\n",
        "\n",
        "# Print some examples\n",
        "# print('Token indices:')\n",
        "# print(val_X[0][:2])\n",
        "# print('Decoded:')\n",
        "# for i in val_X[0][:2]:\n",
        "#     print(tokenizer.decode(list(i)))\n",
        "# print('Segment ids:')\n",
        "# print(val_X[1][:2])\n",
        "# print()\n",
        "# print()\n",
        "\n",
        "# print('Token indices:')\n",
        "# print(train_X[0][:2])\n",
        "# print('Decoded:')\n",
        "# for i in train_X[0][:2]:\n",
        "#     print(tokenizer.decode(list(i)))\n",
        "# print('Segment ids:')\n",
        "# print(train_X[1][:2])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_VwBzKMkbZz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB3kFmUMTIu5"
      },
      "source": [
        "# label_encode = {}\n",
        "# for i, v  in enumerate(train['Label'].unique()):\n",
        "#   label_encode[v] = i\n",
        "# label_encode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah3JpbduTLIp"
      },
      "source": [
        "# train['Label_enc'] = train['Label'].map(label_encode)\n",
        "# val['Label_enc'] = val['Label'].map(label_encode)\n",
        "# val.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLnoTuuuThu2"
      },
      "source": [
        "from keras_bert import load_trained_model_from_checkpoint\n",
        "\n",
        "\n",
        "pretrained_model = load_trained_model_from_checkpoint(\n",
        "    config_file = bert_config_path,\n",
        "    checkpoint_file = bert_checkpoint_path,\n",
        "    training = False,\n",
        "    trainable = True,\n",
        "    seq_len = 256\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReHBEkTVqhqR"
      },
      "source": [
        "# This is a keras model, so we can figure out what inputs it takes like so:\n",
        "pretrained_model.inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL4f_2NWryRb"
      },
      "source": [
        "# And similarly for outputs:\n",
        "pretrained_model.outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrze7NjAr6Sk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "37a546e4-0567-42dd-f893-072382d60477"
      },
      "source": [
        "#@title Print Model Summary\n",
        "\n",
        "pretrained_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-349a617ec391>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@title Print Model Summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpretrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pretrained_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfxKioB9r8Hw"
      },
      "source": [
        "# model.outputs is a list, here with a single item. Here\n",
        "# pretrained_model.outputs[0] just grabs that item (the output tensor).\n",
        "# Indxing that tensor with [:,0] gives the first position in the sequence\n",
        "# for all elements in the batch (the `:`).\n",
        "bert_out = pretrained_model.outputs[0][:,0]\n",
        "\n",
        "print(bert_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp83FyiJsHcK"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "#num_labels = 26\n",
        "\n",
        "dropout_layer = Dropout(.5, input_shape=(768,))(bert_out)\n",
        "out = Dense(num_labels, activation='softmax')(dropout_layer)\n",
        "model = Model(\n",
        "    inputs=pretrained_model.inputs,\n",
        "    outputs=[out]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aJC9vjFsP-T"
      },
      "source": [
        "from keras_bert import calc_train_steps, AdamWarmup\n",
        "\n",
        "\n",
        "# Calculate the number of steps for warmup\n",
        "total_steps, warmup_steps = calc_train_steps(\n",
        "    num_example=len(train['Text'].values),\n",
        "    batch_size=8,\n",
        "    epochs=3,\n",
        "    warmup_proportion=0.1,\n",
        ")\n",
        "\n",
        "optimizer = AdamWarmup(\n",
        "    total_steps,\n",
        "    warmup_steps,\n",
        "    lr=0.00002,\n",
        "    epsilon=1e-6,\n",
        "    weight_decay=0.01,\n",
        "    weight_decay_pattern=['embeddings', 'kernel', 'W1', 'W2', 'Wk', 'Wq', 'Wv', 'Wo']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78kOPfdnsSUc"
      },
      "source": [
        "from keras.metrics import sparse_categorical_accuracy\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['sparse_categorical_accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9Kx_porspDi"
      },
      "source": [
        "# from tensorflow import keras\n",
        "# model = keras.models.load_model('/content/drive/My Drive/Colab Notebooks/assets')\n",
        "history = model.fit(\n",
        "    train_X,\n",
        "    Y,\n",
        "    epochs=3,\n",
        "    batch_size=8,\n",
        "    validation_data= (val_X,y_val)\n",
        "    \n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwJJ1f6UrhQW"
      },
      "source": [
        "train_X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu6WlILP4gTT"
      },
      "source": [
        "# os.chdir('/content/drive/My Drive/Colab Notebooks/')\n",
        "# os.getcwd()\n",
        "# !ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClYwkNhXsrkT"
      },
      "source": [
        "# #!pip install numba \n",
        "# from numba import cuda \n",
        "# device = cuda.get_current_device()\n",
        "# device.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Bf51wMa1LKJ"
      },
      "source": [
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "def plot_history(history):\n",
        "    plt.plot(history.history['sparse_categorical_accuracy'],label=\"Training set accuracy\")\n",
        "    plt.plot(history.history['val_sparse_categorical_accuracy'],label=\"Validation set accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEBKh6ozF86b"
      },
      "source": [
        "model.evaluate(val_X,y_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMWZUWYpvjj2"
      },
      "source": [
        "val_X[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lsVnnUzlgu5"
      },
      "source": [
        "a = model.predict([val_X[0][:5], val_X[1][:5]])\n",
        "a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vl4GkiMwuhHj"
      },
      "source": [
        "a.argmax(axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdpozPAeyWD6"
      },
      "source": [
        "y_val[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeDyC9jxzdH4"
      },
      "source": [
        "\n",
        "[val_X[0][1], val_X[1][1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1W4hmuHzpZ8"
      },
      "source": [
        "from sklearn import metrics\n",
        "matrix = metrics.confusion_matrix(y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8QOtgNRtZxf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}